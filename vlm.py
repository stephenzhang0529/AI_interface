import streamlit as st
import requests
import json
import os

# --- Configuration ---
DEEPSEEK_API_URL = "https://api.siliconflow.cn/v1/chat/completions"
DEEPSEEK_API_KEY = os.environ["API_KEY"]

# Mapping from user-friendly display names to actual VLM model identifiers for the API
# Ensure these actual model identifiers are exactly what the SiliconFlow API expects.
VLM_MODEL_OPTIONS_MAP = {
    "Qwen-VL-72B": "Qwen/Qwen2.5-VL-72B-Instruct",  # More descriptive display name
    "DeepSeek-VL-7B": "deepseek-vl-7b-chat",  # Per your request, API ID is 'deepseek-vl-7b-chat'
    # If the DeepSeek model actually needs 'deepseek-ai/' prefix for API, it would be:
    # "DeepSeek-VL (7B Chat)": "deepseek-ai/deepseek-vl-7b-chat",
}


# Original example values from user's initial code (commented out):
# ("Qwen/QVQ-72B-Preview", "deepseek-ai/deepseek-vl2"),

# --- API Communication Function ---
def asktovlmai(input_text, image_url_val, model_option, max_tokens_val, top_p_val_api):
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    content_parts = []
    if input_text and input_text.strip():
        content_parts.append({"type": "text", "text": input_text})
    if image_url_val and image_url_val.strip():
        content_parts.append({
            "type": "image_url",
            "image_url": {"url": image_url_val, "detail": "auto"}
        })
    if not content_parts:
        st.warning("è¯·è‡³å°‘æä¾›æ–‡æœ¬æˆ–å›¾ç‰‡URLã€‚")
        return "error: no input provided"

    payload = {
        "model": model_option,  # This will use the mapped model ID
        "messages": [{"role": "user", "content": content_parts}],
        "stream": False,
        "max_tokens": max_tokens_val,
        "stop": ["null"],
        "temperature": 0.7,
        "top_p": top_p_val_api,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "response_format": {"type": "text"},
    }

    st.sidebar.write("Request Payload (for debugging):")
    st.sidebar.json(payload)

    try:
        with st.spinner("AI æ­£åœ¨æ€è€ƒä¸­ (VLM)..."):
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        response_data = response.json()
        if response_data.get("choices") and len(response_data["choices"]) > 0 and response_data["choices"][0].get(
                "message"):
            ai_response = response_data["choices"][0]["message"]["content"]
            return ai_response.strip() if ai_response else "error: empty response from AI"
        else:
            st.error("APIå“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–choicesä¸ºç©ºã€‚")
            st.json(response_data)
            return "error: API response format issue"
    except requests.exceptions.Timeout:
        st.error("APIè¯·æ±‚è¶…æ—¶ã€‚è¯·ç¨åå†è¯•ã€‚")
        return "error: request timeout"
    except requests.exceptions.RequestException as e:
        st.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
        if hasattr(e, 'response') and e.response is not None:
            st.error(f"æœåŠ¡å™¨å“åº”ç : {e.response.status_code}")
            st.text(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
            try:
                st.json(e.response.json())
            except json.JSONDecodeError:
                pass
        return "error: request failed"
    except Exception as e:
        st.error(f"å¤„ç†APIè¯·æ±‚æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return "error: unknown processing error"


# --- Streamlit App UI ---
st.set_page_config(page_title="ç™¾å®¶é¥­AI - VLM", layout="wide")
st.title("ç™¾å®¶é¥­AI")

with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")

    # VLM Model selection
    display_vlm_model_names = list(VLM_MODEL_OPTIONS_MAP.keys())
    selected_vlm_display_name = st.selectbox(
        "é€‰æ‹©æ¨¡å‹:",
        options=display_vlm_model_names,
        index=0,
        placeholder="é€‰æ‹©ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹...",
        key="vlm_model_select_display",
        help="é€‰æ‹©æ‚¨æƒ³ä½¿ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚"
    )
    # The actual model identifier to be used in API calls
    model_selected_vlm = VLM_MODEL_OPTIONS_MAP[selected_vlm_display_name]

    st.subheader("ğŸ› ï¸ å‚æ•°è°ƒæ•´")
    max_token_val = st.slider(
        "Max Tokens:",
        min_value=50, max_value=4096, value=1024, step=64,
        key="vlm_max_token_slider",
        help="AIå›å¤çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥tokensè®¡ç®—ï¼‰ã€‚"
    )
    top_p_val = st.slider(
        "Temperature:",
        min_value=0.0, max_value=1.0, value=0.7, step=0.01,
        key="vlm_top_p_slider",
        help="æ§åˆ¶è¾“å‡ºæ–‡æœ¬çš„éšæœºæ€§ã€‚\n(APIçš„temperatureå‚æ•°å›ºå®šä¸º0.7)"
    )


st.markdown("---")
st.subheader("ğŸ–¼ï¸ å›¾åƒè¾“å…¥")
image_url_input = st.text_input(
    "è¯·è¾“å…¥å›¾ç‰‡çš„URLåœ°å€:",
    placeholder="ä¾‹å¦‚: https://example.com/image.jpg",
    key="image_url_field"
)
if image_url_input:
    if image_url_input.startswith("http://") or image_url_input.startswith("https://"):
        st.image(image_url_input, caption="æ‚¨æä¾›çš„å›¾ç‰‡", width=300)
    else:
        st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„å›¾ç‰‡URL (ä»¥ http:// æˆ– https:// å¼€å¤´)ã€‚")
        image_url_input = None

st.markdown("---")
st.subheader("ğŸ’¬ æé—®")
text_input_data = st.chat_input("å…³äºå›¾ç‰‡ï¼Œæ‚¨æƒ³é—®ä»€ä¹ˆï¼Ÿæˆ–è€…ç›´æ¥æè¿°ä»»åŠ¡ã€‚")

if text_input_data or (image_url_input and not text_input_data):  # Allow if text OR (image and no text yet)
    if not image_url_input:
        st.warning("è¯·æä¾›å›¾ç‰‡URLä»¥ä¾¿è¿›è¡Œè§†è§‰é—®ç­”ã€‚")
    else:
        user_message_display = ""
        if image_url_input:
            user_message_display += f"å›¾ç‰‡URL: {image_url_input}\n"
        if text_input_data:  # Will be non-empty if this branch is taken due to text_input_data
            user_message_display += f"é—®é¢˜: {text_input_data}"
        elif image_url_input:  # Only image was provided, text_input_data is empty
            user_message_display += f"é—®é¢˜: (æ— æ–‡æœ¬è¾“å…¥ï¼Œå°è¯•åˆ†æå›¾ç‰‡)"

        with st.chat_message("user"):
            st.markdown(user_message_display.strip())

        ai_response = asktovlmai(
            input_text=text_input_data if text_input_data else "",
            image_url_val=image_url_input,
            model_option=model_selected_vlm,  # Use the mapped VLM model ID
            max_tokens_val=max_token_val,
            top_p_val_api=top_p_val
        )

        if ai_response and not ai_response.startswith("error:"):
            with st.chat_message("ai"):
                st.markdown(ai_response)

# This separate button logic might become redundant if chat_input handles the "enter" key well
# even when it's empty but other fields are filled. Consider simplifying if not strictly needed.
elif st.button("å‘é€", key="send_button_vlm", help="å¦‚æœæ‚¨åªè¾“å…¥äº†URLï¼Œç‚¹å‡»æ­¤å¤„å‘é€è¿›è¡Œåˆ†æï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰ã€‚"):
    if image_url_input and not text_input_data:
        st.info("å°†å°è¯•åˆ†æå›¾ç‰‡ã€‚å»ºè®®ä¹Ÿè¾“å…¥ä¸€ä¸ªé—®é¢˜æˆ–æŒ‡ä»¤ã€‚")
        with st.chat_message("user"):
            st.markdown(f"å›¾ç‰‡URL: {image_url_input}\né—®é¢˜: (æ— æ–‡æœ¬è¾“å…¥ï¼Œå°è¯•åˆ†æå›¾ç‰‡)")
        ai_response = asktovlmai(
            input_text="",
            image_url_val=image_url_input,
            model_option=model_selected_vlm,
            max_tokens_val=max_token_val,
            top_p_val_api=top_p_val
        )
        if ai_response and not ai_response.startswith("error:"):
            with st.chat_message("ai"):
                st.markdown(ai_response)
    elif not image_url_input:
        st.warning("è¯·è¾“å…¥å›¾ç‰‡URLã€‚")
    else:  # Text input exists, but button was clicked instead of enter in chat_input
        st.info("è¯·é€šè¿‡ä¸Šæ–¹çš„è¾“å…¥æ¡†æé—®ï¼Œæˆ–ç›´æ¥æŒ‰å›è½¦ã€‚")